{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEmAuaoDxSi2"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import heapq\n",
        "import json\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "\n",
        "INF = 1e12  # Represents infinity for cost\n",
        "os.system('cls' if os.name == 'nt' else 'clear')\n",
        "\n",
        "# Input/output directories\n",
        "input_folder = r'C:\\Users\\Murilo\\Documents\\DOUTORADO\\UFMS-FACOM\\Material-Ricardo\\Encontros-Ricardo\\Encontro_55-08-07-2025\\25nds\\input'\n",
        "output_folder = r'C:\\Users\\Murilo\\Documents\\DOUTORADO\\UFMS-FACOM\\Material-Ricardo\\Encontros-Ricardo\\Encontro_55-08-07-2025\\25nds\\output'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "PARQUET_COST_DIR = os.path.join(output_folder, 'cost_matrix_parquet')\n",
        "DATA_DIR = os.path.join(output_folder, 'parquet_r2_jobs')\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_COST_DIR, exist_ok=True)\n",
        "\n",
        "# Compute shortest path latency from source using Dijkstra\n",
        "def get_latencies(source, adjList, numNodes):\n",
        "    latencies = [INF] * numNodes\n",
        "    latencies[source] = 0\n",
        "    heap = [(0, source)]\n",
        "    while heap:\n",
        "        curr_lat, u = heapq.heappop(heap)\n",
        "        if curr_lat > latencies[u]:\n",
        "            continue\n",
        "        for v, weight in adjList[u]:\n",
        "            if latencies[v] > curr_lat + weight:\n",
        "                latencies[v] = curr_lat + weight\n",
        "                heapq.heappush(heap, (latencies[v], v))\n",
        "    return latencies\n",
        "\n",
        "# Save valid combinations to a Parquet file\n",
        "def save_comb_to_parquet(job_id, comb_list):\n",
        "    if not comb_list:\n",
        "        df = pd.DataFrame(columns=[\"node1\"])\n",
        "    else:\n",
        "        max_len = max(len(c) for c in comb_list)\n",
        "        col_names = [f'node{i+1}' for i in range(max_len)]\n",
        "        df = pd.DataFrame(comb_list, columns=col_names)\n",
        "    table = pa.Table.from_pandas(df)\n",
        "    pq.write_table(table, os.path.join(DATA_DIR, f'job_{job_id:05d}.parquet'), compression='snappy')\n",
        "\n",
        "# Generate all valid node combinations of size 1 and 2 and save them\n",
        "def generate_combinations_r1_r2_and_save(data):\n",
        "    jr, jb, jl, jo = map(np.array, (data['jr'], data['jb'], data['jl'], data['jo']))  # Job requirements\n",
        "    V_R, V_B = map(np.array, (data['V_R'], data['V_B']))  # Node capabilities\n",
        "    adjList = data['adjList']\n",
        "    num_nodes = len(V_R)\n",
        "    num_jobs = len(jr)\n",
        "    print(f\"\\nRunning Scenario: {num_nodes} nds - {num_jobs} jobs. Waiting...\\n\")\n",
        "\n",
        "    for job_id in tqdm(range(num_jobs), desc=\"Generating and saving combinations\"):\n",
        "        comb_validas = []\n",
        "        jrj, jbj, jlj, joj = jr[job_id], jb[job_id], jl[job_id], jo[job_id]\n",
        "        N_Latency = get_latencies(joj, adjList, num_nodes)\n",
        "        for r in [1, 2]:  # Only 1-node and 2-node combinations\n",
        "            for comb in itertools.combinations(range(num_nodes), r):\n",
        "                total_R = sum(V_R[n] for n in comb)\n",
        "                total_B = sum(V_B[n] for n in comb)\n",
        "                total_L = sum(N_Latency[n] for n in comb)\n",
        "                if (total_R >= jrj and total_B >= jbj and total_L <= jlj - 1):\n",
        "                    comb_validas.append(tuple(comb))\n",
        "        save_comb_to_parquet(job_id, comb_validas)\n",
        "        gc.collect()\n",
        "    return num_nodes, num_jobs\n",
        "\n",
        "# Lazily load combinations from Parquet to reduce memory usage\n",
        "def load_parquet_combinations_lazy(job_id):\n",
        "    path = os.path.join(DATA_DIR, f'job_{job_id:05d}.parquet')\n",
        "    pf = pq.ParquetFile(path)\n",
        "    for batch in pf.iter_batches():\n",
        "        df = batch.to_pandas()\n",
        "        for row in df.itertuples(index=False):\n",
        "            yield tuple(int(v) for v in row if not pd.isna(v))\n",
        "\n",
        "# Save job cost matrix to Parquet\n",
        "def save_cost_matrix_to_parquet(job_id, costs_row, combs):\n",
        "    df = pd.DataFrame({'comb': list(map(str, combs)), 'cost': costs_row})\n",
        "    table = pa.Table.from_pandas(df)\n",
        "    pq.write_table(table, os.path.join(PARQUET_COST_DIR, f'job_{job_id:05d}.parquet'), compression='snappy')\n",
        "\n",
        "# Compute and store cost matrix for all jobs\n",
        "def calculate_cost_matrix_streaming(data):\n",
        "    jr, jb, jl, jo = map(np.array, (data['jr'], data['jb'], data['jl'], data['jo']))\n",
        "    V_R, V_B = map(np.array, (data['V_R'], data['V_B']))\n",
        "    adjList = data['adjList']\n",
        "    num_jobs = len(jr)\n",
        "    num_nodes = len(V_R)\n",
        "    unique_combs_set = set()\n",
        "    for job_id in range(num_jobs):\n",
        "        for comb in load_parquet_combinations_lazy(job_id):\n",
        "            unique_combs_set.add(tuple(sorted(comb)))\n",
        "    unique_combs = list(unique_combs_set)\n",
        "    latency_cache = {o: get_latencies(o, adjList, num_nodes) for o in np.unique(jo)}\n",
        "    for job_id in tqdm(range(num_jobs), desc=\"Saving cost matrix in parquet\"):\n",
        "        origin = jo[job_id]\n",
        "        l_job = jl[job_id] - 1\n",
        "        N_Latency = latency_cache[origin]\n",
        "        row_costs = []\n",
        "        for comb in unique_combs:\n",
        "            total_R = sum(V_R[n] for n in comb)\n",
        "            total_B = sum(V_B[n] for n in comb)\n",
        "            total_L = sum(N_Latency[n] for n in comb)\n",
        "            if total_R < jr[job_id] or total_B < jb[job_id] or total_L > l_job:\n",
        "                row_costs.append(INF)\n",
        "            else:\n",
        "                cost = (total_R - jr[job_id]) ** 2 + (total_B - jb[job_id]) ** 2 - (l_job - total_L)\n",
        "                row_costs.append(cost)\n",
        "        save_cost_matrix_to_parquet(job_id, row_costs, unique_combs)\n",
        "        gc.collect()\n",
        "    return unique_combs, jr, jb, jl, jo\n",
        "\n",
        "# Load cost vector for a job\n",
        "def load_job_costs_parquet(job_id):\n",
        "    df = pd.read_parquet(os.path.join(PARQUET_COST_DIR, f'job_{job_id:05d}.parquet'))\n",
        "    return df['cost'].values.tolist()\n",
        "\n",
        "# Estimate lower bound of cost for remaining jobs (used in BnB)\n",
        "def calculate_lower_bound_parquet(start_job, assigned_nodes, combs, n_jobs):\n",
        "    lb = 0\n",
        "    for j in range(start_job, n_jobs):\n",
        "        costs = load_job_costs_parquet(j)\n",
        "        filtered = [c for idx, c in enumerate(costs) if not set(combs[idx]).intersection(assigned_nodes)]\n",
        "        if filtered:\n",
        "            lb += min(filtered)\n",
        "        else:\n",
        "            return INF\n",
        "    return lb\n",
        "\n",
        "# Branch and Bound optimization algorithm\n",
        "def branch_and_bound_parquet(combs, n_jobs):\n",
        "    best_cost = INF\n",
        "    best_solution = None\n",
        "    heap = [(0, [], 0, set())]  # (lower bound, partial sol, cost, assigned nodes)\n",
        "    while heap:\n",
        "        lb, partial, curr_cost, assigned = heapq.heappop(heap)\n",
        "        job = len(partial)\n",
        "        if job == n_jobs:\n",
        "            if curr_cost < best_cost:\n",
        "                best_cost = curr_cost\n",
        "                best_solution = partial\n",
        "            continue\n",
        "        job_costs = load_job_costs_parquet(job)\n",
        "        for idx, comb in enumerate(combs):\n",
        "            if set(comb).intersection(assigned):\n",
        "                continue\n",
        "            new_cost = curr_cost + job_costs[idx]\n",
        "            new_assigned = assigned.union(comb)\n",
        "            est_lb = new_cost + calculate_lower_bound_parquet(job + 1, new_assigned, combs, n_jobs)\n",
        "            if est_lb >= best_cost:\n",
        "                continue\n",
        "            heapq.heappush(heap, (est_lb, partial + [(job, idx)], new_cost, new_assigned))\n",
        "    if best_solution is None:\n",
        "        return None, None\n",
        "    return [(j, combs[idx]) for j, idx in best_solution], [load_job_costs_parquet(j)[idx] for j, idx in best_solution]\n",
        "\n",
        "# Print and save results to output file\n",
        "def print_summary(solution, costs, jr, jb, jl, jo, input_path, output_folder, runtime, num_nodes, num_jobs):\n",
        "    input_file = os.path.basename(input_path).split('.')[0]\n",
        "    out_path = os.path.join(output_folder, f\"results_parquet_{input_file}.txt\")\n",
        "    with open(out_path, 'w') as f:\n",
        "        f.write(\"-\" * 62 + \"\\n\")\n",
        "        f.write(\"-\" * 22 + \" Branch and Bound \" + \"-\" * 22 + \"\\n\")\n",
        "        f.write(\"-\" * 62 + \"\\n\")\n",
        "        f.write(f\" Scenario: {num_nodes} nds - {num_jobs} jobs\\n\")\n",
        "        f.write(\"-\" * 62 + \"\\n\")\n",
        "        f.write(f\"Input file: {input_file}\\n\\n\")\n",
        "        f.write(\" Job   [ Jr,  Jb,  Jl,  Jo]\".ljust(50) + \"OF\".rjust(20) + \"Allocated Nodes\".rjust(25) + \"\\n\")\n",
        "\n",
        "        total_cost = 0.0\n",
        "        for i, (job_idx, nodes) in enumerate(solution):\n",
        "            cost = costs[i]\n",
        "            total_cost += cost\n",
        "            f.write(f\"  {job_idx:<3}  [{int(jr[job_idx])}, {int(jb[job_idx])}, {int(jl[job_idx])}, {int(jo[job_idx])}]\".ljust(50))\n",
        "            f.write(f\"{cost:>20,.1f}\".rjust(20) + f\"{str(list(nodes)).rjust(25)}\\n\")\n",
        "\n",
        "        f.write(f\"\\nTotal OF: {total_cost:,.1f}\\n\")\n",
        "        f.write(f\"\\nRuntime: {runtime:,.4f} seconds\\n\")\n",
        "\n",
        "    print(f\"\\nüìÅ Results saved in: {out_path}\")\n",
        "\n",
        "# Main execution logic\n",
        "def main():\n",
        "    for json_file in [f for f in os.listdir(input_folder) if f.endswith('.json')]:\n",
        "        input_path = os.path.join(input_folder, json_file)\n",
        "        with open(input_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        start_total = time.time()\n",
        "        num_nodes, num_jobs = generate_combinations_r1_r2_and_save(data)\n",
        "        combs_all, jr, jb, jl, jo = calculate_cost_matrix_streaming(data)\n",
        "        solution, costs = branch_and_bound_parquet(combs_all, num_jobs)\n",
        "        end_total = time.time()\n",
        "        print_summary(solution, costs, jr, jb, jl, jo, input_path, output_folder, end_total - start_total, num_nodes, num_jobs)\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}