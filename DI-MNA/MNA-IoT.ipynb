{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEmAuaoDxSi2"
      },
      "outputs": [],
      "source": [
        "''' MNA-IoT-DI\n",
        "It runs, from a .json input file, generating solutions (stored in .txt files),\n",
        "as well as a new .json output file containing the parameters of the best solution\n",
        "found by this algorithm.\n",
        "\n",
        "Designed by Murilo Táparo - November 2023\n",
        "\n",
        "Last modified: 06/27/2025'''\n",
        "# -*- coding: utf-8 -*-\n",
        "# MNA-IoT mem optimization\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import heapq\n",
        "import gc\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "\n",
        "#---------------------------- Global variables ---------------------------------\n",
        "INF = 10**12 # Infinite\n",
        "t_c = 1 # Time connection (t_c=1 ms)\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def vector_space_generator(filtered_nodes, numNodes, cut_sol):\n",
        "    \"\"\"\n",
        "    Generates binary masks with up to 'cut_sol' active nodes, only at the indexes in filtered_nodes.\n",
        "\n",
        "    Args:\n",
        "    filtered_nodes (list[int]): Indexes of the nodes considered.\n",
        "    numNodes (int): Total nodes in the network.\n",
        "    cut_sol (int): Maximum active nodes per combination.\n",
        "\n",
        "    Yields:\n",
        "    list[int]: Binary mask with 1s in the active nodes of the combination.\n",
        "    \"\"\"\n",
        "    for k in range(1, cut_sol + 1):  # combinações de 1 até cut_sol nós ativos\n",
        "        for combo in combinations(filtered_nodes, k):\n",
        "            mask = [0] * numNodes\n",
        "            for i in combo:\n",
        "                mask[i] = 1\n",
        "            yield mask\n",
        "\n",
        "# Based on Dijkstra's Algorithm\n",
        "def get_latencies(source, adjList, numNodes):\n",
        "    latencies= [INF] * numNodes\n",
        "    latencies[source] = 0\n",
        "    heap = [(0, source)]\n",
        "\n",
        "    while heap:\n",
        "        curr_lat, u = heapq.heappop(heap)\n",
        "        if curr_lat > latencies[u]:\n",
        "            continue\n",
        "        for v, weight in adjList[u]:\n",
        "            if latencies[v] > curr_lat + weight:\n",
        "                latencies[v] = curr_lat + weight\n",
        "                heapq.heappush(heap, (latencies[v], v))\n",
        "    return latencies\n",
        "\n",
        "\n",
        "# Prints to the Terminal the execution header of each input file and the current job\n",
        "#def print_header(file, numRunnings, r, job, jr, jb, jl, jo):\n",
        "    #---------------- Clear terminal window before running each Job ---------------\n",
        "    #os.system('cls' if os.name == 'nt' else 'clear')\n",
        "    #print(\"\\n---------------------------------------------------------\")\n",
        "    #print(f\"\\n Processing the file: {file}\")\n",
        "    #print(f\"\\n Running: {r+1}/{numRunnings}\")\n",
        "    #print(f\"\\n Job {job} [{jr[job]}, {jb[job]}, {jl[job]}, {jo[job]}]\")\n",
        "    #print(\"\\n---------------------------------------------------------\\n\")\n",
        "#    return\n",
        "\n",
        "def preselect_nodes(available, N_R, N_B, N_L, jr_job, jb_job, l_job, numNodes, cut_comb_nodes, cut_sol):\n",
        "    \"\"\"\n",
        "    Preselects combinations of up to `cut_sol` nodes that together satisfy job requirements,\n",
        "    and returns up to `cut_comb_nodes` combinations sorted by minimum max latency.\n",
        "\n",
        "    Parameters:\n",
        "    available (list[bool]): Availability status of nodes.\n",
        "    N_R (list[float]): Resource availability per node.\n",
        "    N_B (list[float]): Bandwidth availability per node.\n",
        "    N_L (list[float]): Latency per node.\n",
        "    jr_job (float): Job resource requirement.\n",
        "    jb_job (float): Job bandwidth requirement.\n",
        "    l_job (float): Job latency constraint.\n",
        "    numNodes (int): Total number of nodes.\n",
        "    cut_comb_nodes (int): Maximum number of combinations to return.\n",
        "    cut_sol (int): Maximum number of nodes allowed in a combination.\n",
        "\n",
        "    Returns:\n",
        "    list[tuple[int]]: List of node combinations satisfying the job.\n",
        "    \"\"\"\n",
        "    valid_combinations = []\n",
        "    # Generates all combinations of available IoT network nodes\n",
        "    candidates = [i for i in range(numNodes) if available[i]]\n",
        "\n",
        "    for r in range(1, cut_sol + 1):  # Quantity combinations from 1 to cut_sol\n",
        "        # Cut Radius (Job Latency), latency threshold\n",
        "        for combo in combinations(candidates[:cut_comb_nodes], r):\n",
        "            total_R = sum(N_R[i] for i in combo)\n",
        "            total_B = sum(N_B[i] for i in combo)\n",
        "            max_L = max(N_L[i] for i in combo)\n",
        "\n",
        "            if total_R >= jr_job and total_B >= jb_job and max_L <= l_job:\n",
        "                valid_combinations.append((combo, max_L))\n",
        "\n",
        "    # Only take the first `cut_comb_nodes` combinations\n",
        "    top_combinations = [combo for combo, _ in valid_combinations[:cut_comb_nodes]]\n",
        "\n",
        "    return top_combinations\n",
        "#-----------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_mna_jobs(file, numRunnings, r, jr, jb, jl, jo, N_R, N_B, adjList, numNodes):\n",
        "    n_jobs = len(jr)\n",
        "    l_job = 0\n",
        "    v_all_OF = []\n",
        "    v_all_nodes = []\n",
        "    v_all_sol_feasible = []\n",
        "    available= [True] * numNodes  # Allocated nodes management. In the beginning, all nodes are available\n",
        "\n",
        "    for job in range(n_jobs):\n",
        "        # Prints to the Terminal the execution header of each input file and the current job\n",
        "        #print_header(file, numRunnings, r, job, jr, jb, jl, jo)\n",
        "        source = jo[job]  # Stores the position value of the source node\n",
        "        # Discounts the initial node connection time in the calculation (only 1 time for each job)\n",
        "        l_job = jl[job] - t_c\n",
        "        N_L = get_latencies(source, adjList, numNodes)\n",
        "\n",
        "        Min_FX = INF\n",
        "        better_mask = [0] * numNodes\n",
        "        v_sol_feasible = 0 # Nº feasible solutions\n",
        "        #---------------------------------------------------------------------------------------------------------------------\n",
        "        # Hyperparameters for cutting combinations considered and maximum number of nodes in the generated solutions\n",
        "        # cut_comb_nodes = 500 # Considers the cut_comb_nodes combinations, according to DI, for each job.\n",
        "        # cut_sol = 2 # Considers a maximum of 2 nodes when allocating a job\n",
        "        #---------------------------------------------------------------------------------------------------------------------\n",
        "        filtered_nodes = preselect_nodes(available, N_R, N_B, N_L, jr[job], jb[job], l_job, numNodes, cut_comb_nodes, cut_sol)\n",
        "        # Flatten combinations to get only node indices (Ordered according to DI criteria)\n",
        "        filtered_nodes = set(i for combo in filtered_nodes for i in combo)\n",
        "        # Reduces the search space to only the filtered nodes (vector_space_generator=filtered_nodes)\n",
        "        for mask in vector_space_generator(filtered_nodes, numNodes, cut_sol):\n",
        "            comb_nodes = [i for i in filtered_nodes if mask[i]]\n",
        "            # Skip the current iteration of the loop if the comb_nodes list is empty\n",
        "            if not comb_nodes:\n",
        "               continue\n",
        "            # Discards combinations that:\n",
        "            # (1) not all(available[i] for i in comb_nodes) - have some unavailable nodes, OR\n",
        "            # (2) len(comb_nodes) > 2 - have more than 2 active nodes\n",
        "            if not all(available[i] for i in comb_nodes) or len(comb_nodes) > 2:\n",
        "               continue\n",
        "\n",
        "            sum_R = sum(N_R[i] for i in comb_nodes)\n",
        "            sum_B = sum(N_B[i] for i in comb_nodes)\n",
        "            sum_L = sum(N_L[i] for i in comb_nodes)\n",
        "\n",
        "            if (sum_R >= jr[job] and sum_B >= jb[job] and sum_L <= l_job):\n",
        "                f0 = sum_R - jr[job]\n",
        "                f1 = sum_B - jb[job]\n",
        "                f2 = l_job - sum_L\n",
        "                OF = f0**2 + f1**2 - f2\n",
        "                v_sol_feasible += 1\n",
        "                if OF <= Min_FX:\n",
        "                    Min_FX = OF\n",
        "                    better_mask = mask\n",
        "\n",
        "            else:\n",
        "                # For solutions that are not feasible, move on to the next configuration\n",
        "                continue\n",
        "\n",
        "        #---------------------------------------- End for mask -------------------------------------------------\n",
        "        #-------------- Stores the number of feasible solutions for each job -----------------------------------\n",
        "        v_all_sol_feasible.append(v_sol_feasible)\n",
        "\n",
        "        if Min_FX < INF:\n",
        "            allocated = [i for i in range(numNodes) if better_mask[i]]\n",
        "            for i in allocated:\n",
        "                available[i] = False\n",
        "            v_all_OF.append(Min_FX)\n",
        "            v_all_nodes.append(allocated)\n",
        "        else:\n",
        "            v_all_OF.append(0)\n",
        "            v_all_nodes.append([])\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "    return v_all_OF, v_all_nodes, v_all_sol_feasible\n",
        "\n",
        "\n",
        "def run_mna_iot_batch(source_dir, target_dir, numRunnings):\n",
        "    \"\"\"\n",
        "    Reads files generated by the save_nodes_graph_to_parquet function\n",
        "    and executes the MNA-IoT algorithm in batch mode for all datasets found.\n",
        "\n",
        "    Parameters:\n",
        "    source_dir (str): directory where the generated files are located.\n",
        "    target_dir (str): directory where the results will be saved.\n",
        "    numRunnings (int): number of executions.\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    all_files = os.listdir(source_dir)\n",
        "\n",
        "    # Configuration types\n",
        "    type_suffix = {\"lightweight\": \"_light\", \"heavyweight\": \"_heavy\"}\n",
        "\n",
        "    for cfg, suffix in type_suffix.items():\n",
        "        # finds all job files for this configuration\n",
        "        job_files = [f for f in all_files if f.endswith(f\"{suffix}.parquet\") and \"jobs\" in f]\n",
        "\n",
        "        if not job_files:\n",
        "            print(f\"⚠️ No jobs files found for {cfg}\")\n",
        "            continue\n",
        "\n",
        "        for jobs_file in job_files:\n",
        "            full_base = jobs_file.replace(f\"{suffix}.parquet\", \"\")\n",
        "            #print(f\"\\n📁 Dataset detectado: {full_base} ({cfg})\")\n",
        "\n",
        "            # Corresponding info file\n",
        "            info_file = f\"{full_base}{suffix}_info.parquet\"\n",
        "            if info_file not in all_files:\n",
        "                print(f\"⚠️ Missing info file: {info_file}\")\n",
        "                continue\n",
        "\n",
        "            # Find correct nodes and edges_manifest by dataset prefix\n",
        "            dataset_prefix = \"_\".join(full_base.split(\"_\")[:2])\n",
        "            nodes_file_candidates = [f for f in all_files if f.startswith(dataset_prefix) and f.endswith(\"_nodes.parquet\")]\n",
        "            manifest_file_candidates = [f for f in all_files if f.startswith(dataset_prefix) and f.endswith(\"_edges_manifest.parquet\")]\n",
        "\n",
        "            if not nodes_file_candidates or not manifest_file_candidates:\n",
        "                print(f\"⚠️ Missing nodes/manifest files for {dataset_prefix}\")\n",
        "                continue\n",
        "\n",
        "            nodes_file = nodes_file_candidates[0]\n",
        "            manifest_file = manifest_file_candidates[0]\n",
        "\n",
        "            # Absolute Paths\n",
        "            path_jobs = os.path.join(source_dir, jobs_file)\n",
        "            path_info = os.path.join(source_dir, info_file)\n",
        "            path_nodes = os.path.join(source_dir, nodes_file)\n",
        "            path_manifest = os.path.join(source_dir, manifest_file)\n",
        "\n",
        "            # Reads main files\n",
        "            df_jobs = pd.read_parquet(path_jobs)\n",
        "            df_info = pd.read_parquet(path_info)\n",
        "            df_nodes = pd.read_parquet(path_nodes)\n",
        "            df_manifest = pd.read_parquet(path_manifest)\n",
        "\n",
        "            # Reads edge partitions, if any.\n",
        "            df_edges_list = []\n",
        "            if \"file\" in df_manifest.columns:\n",
        "                for f in df_manifest[\"file\"]:\n",
        "                    full_path = os.path.join(source_dir, os.path.basename(f))\n",
        "                    if os.path.exists(full_path):\n",
        "                        df_edges_list.append(pd.read_parquet(full_path))\n",
        "            else:\n",
        "                df_edges_list.append(df_manifest)\n",
        "\n",
        "            if not df_edges_list:\n",
        "                print(f\"❌ No edge partitions loaded for {full_base}{suffix}\")\n",
        "                continue\n",
        "\n",
        "            df_edges = pd.concat(df_edges_list, ignore_index=True)\n",
        "\n",
        "            # Variable extraction\n",
        "            jr = df_jobs[\"jr\"].to_numpy()\n",
        "            jb = df_jobs[\"jb\"].to_numpy()\n",
        "            jl = df_jobs[\"jl\"].to_numpy()\n",
        "            jo = df_jobs[\"jo\"].to_numpy()\n",
        "\n",
        "            V_R = df_nodes[\"R\"].to_numpy()\n",
        "            V_B = df_nodes[\"B\"].to_numpy()\n",
        "            V_Busy = df_nodes[\"Busy\"].to_numpy()\n",
        "            V_Inactive = df_nodes[\"Inactive\"].to_numpy()\n",
        "            numNodes = len(V_R)\n",
        "            edge_nodes = df_info[\"edge_nodes\"].iloc[0]\n",
        "\n",
        "            # Create adjacency list\n",
        "            adjList = [[] for _ in range(numNodes)]\n",
        "            for _, row in df_edges.iterrows():\n",
        "                src, tgt, lat = int(row[\"source\"]), int(row[\"target\"]), float(row[\"latency\"])\n",
        "                adjList[src].append((tgt, lat))\n",
        "\n",
        "            # Job ordering\n",
        "            c0, c1, c2 = 60, 1, 39\n",
        "            ordered_lists = sorted(zip(jr, jb, jl, jo),\n",
        "                                   key=lambda x: ((c0*(0.253*x[0])+c1*(0.024*x[1])-c2*(0.723*x[2]))/(c0+c1+c2)),\n",
        "                                   reverse=True)\n",
        "            jr, jb, jl, jo = map(list, zip(*ordered_lists))\n",
        "\n",
        "            # Executions\n",
        "            times_execs = []\n",
        "            for r in range(numRunnings):\n",
        "                start = time.process_time()\n",
        "                v_all_OF, v_all_nodes, v_all_sol_feasible = run_mna_jobs(\n",
        "                    full_base, numRunnings, r, jr, jb, jl, jo, V_R, V_B, adjList, numNodes)\n",
        "                runtime = time.process_time() - start\n",
        "                times_execs.append(runtime)\n",
        "\n",
        "                # Salva resultados em .txt e estatísticas na mesma abertura do arquivo\n",
        "                def format_float(x): return f\"{x:18,.1f}\"\n",
        "                output_path = os.path.join(target_dir, f\"results_{r}_MNA_IoT_{full_base}{suffix}.txt\")\n",
        "                with open(output_path, 'w', encoding=\"utf-8\") as out:\n",
        "                    out.write(f\"Input file: {jobs_file}\\nNumber of jobs: {len(jr)}\\n\\n\")\n",
        "                    rows = (\n",
        "                        [i, f\"[{jr[i]}, {jb[i]}, {jl[i]}, {jo[i]}]\", format_float(of), str(sorted(nodes))]\n",
        "                        for i, (of, nodes) in enumerate(zip(v_all_OF, v_all_nodes))\n",
        "                    )\n",
        "                    table = tabulate(rows, headers=[\"Job\", \"[Jr,Jb,Jl,Jo]\", \"OF\", \"Allocated nodes\"], tablefmt=\"plain\")\n",
        "                    out.write(table)\n",
        "                    out.write(f\"\\n\\nTotal OF: {format_float(np.sum(v_all_OF)).strip()}\\nRuntime: {runtime:.5f} sec\\n\")\n",
        "\n",
        "                    # Última execução: salva arquivos parquet e estatísticas dentro do mesmo with\n",
        "                    if r == numRunnings - 1:\n",
        "                        prefix = os.path.join(target_dir, f\"MNA_IoT_{full_base}{suffix}\")\n",
        "\n",
        "                        df_info_out = pd.DataFrame({\"edge_nodes\": [edge_nodes]})\n",
        "                        pq.write_table(pa.Table.from_pandas(df_info_out), f\"{prefix}_info.parquet\")\n",
        "\n",
        "                        df_jobs_out = pd.DataFrame({\"jr\": jr, \"jb\": jb, \"jl\": jl, \"jo\": jo})\n",
        "                        pq.write_table(pa.Table.from_pandas(df_jobs_out), f\"{prefix}_jobs.parquet\")\n",
        "\n",
        "                        df_nodes_out = pd.DataFrame({\"R\": V_R, \"B\": V_B, \"Busy\": V_Busy, \"Inactive\": V_Inactive})\n",
        "                        pq.write_table(pa.Table.from_pandas(df_nodes_out), f\"{prefix}_nodes.parquet\")\n",
        "\n",
        "                        edge_data = []\n",
        "                        for src, neighbors in enumerate(adjList):\n",
        "                            for tgt, lat in neighbors:\n",
        "                                edge_data.append((src, tgt, lat))\n",
        "                        df_edges_out = pd.DataFrame(edge_data, columns=[\"source\", \"target\", \"latency\"])\n",
        "                        pq.write_table(pa.Table.from_pandas(df_edges_out), f\"{prefix}_edges.parquet\")\n",
        "\n",
        "                        df_results_out = pd.DataFrame({\n",
        "                            \"v_all_OF\": v_all_OF,\n",
        "                            \"v_all_nodes\": [json.dumps(n) for n in v_all_nodes],\n",
        "                            \"v_all_sol_feasible\": v_all_sol_feasible,\n",
        "                            \"Min_FX\": [sum(v_all_OF)] * len(v_all_OF)\n",
        "                        })\n",
        "                        pq.write_table(pa.Table.from_pandas(df_results_out), f\"{prefix}_results.parquet\")\n",
        "\n",
        "                        # Estatísticas de todas as execuções\n",
        "                        out.write(\"\\n--------------------- Times Execs -------------------------------\")\n",
        "                        for i, t in enumerate(times_execs):\n",
        "                            out.write(f\"\\n {i:4d}:  {t:,.5f}\")\n",
        "                        mean, sd = np.mean(times_execs), np.std(times_execs, ddof=(0 if numRunnings==1 else 1))\n",
        "                        out.write(f\"\\n mean: {mean:,.5f}\\n   sd: {sd:,.5f}\\n\")\n",
        "                        out.write(\"-----------------------------------------------------------------\\n\")\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Start of execution (main):\n",
        "\n",
        "#---------------- Clear terminal window before program execution ---------------\n",
        "os.system('cls' if os.name == 'nt' else 'clear')\n",
        "# On Windows (nt) systems, use the cls command\n",
        "# On Unix/Linux and MacOS systems, use the clear command\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Setting default values (global)\n",
        "cut_comb_nodes = 500\n",
        "cut_sol = 2\n",
        "\n",
        "# Set the directory path where the .json files are located\n",
        "# Setting default values\n",
        "source_dir = '/home/jonatas/high-performance-execution/ExperimentSix/improved-execution-sentinel/Configs_Jonatas/10_000nds/'\n",
        "target_dir = '/home/jonatas/high-performance-execution/ExperimentSix/improved-execution-sentinel/Configs_Jonatas/output-10_000nds/'\n",
        "\n",
        "\n",
        "# Number of runnings of a given configuration\n",
        "numRunnings = 10\n",
        "\n",
        "# Create the output folder if it doesn't exist; keep if it already exists\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "run_mna_iot_batch(source_dir, target_dir, numRunnings)\n"
      ]
    }
  ]
}